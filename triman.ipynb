{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch sur des séquences de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext # pip install torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commandes utiles en cas de lib manquantes :\n",
    "\n",
    "# !pip3 install -U spacy                       # spaCy lib\n",
    "# !pip3 install torch torchvision torchaudio   # PyTorch (see <pytorch.org/get-started/locally> for GPU support)\n",
    "# !python -m spacy download en_core_web_trf    # download spaCy trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\antonin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# TODO: find a way to merge EN & FR language pipelines\n",
    "spacy_en = spacy.load('en_core_web_trf')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des prétraitements sur le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, lower = True, include_lengths = False,\n",
    "            pad_token = \"<pad>\", unk_token = \"<unk>\",\n",
    "            batch_first = True, tokenize = tokenizer)\n",
    "\n",
    "LABELS = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'london',\n",
       " '#',\n",
       " 'microsoft',\n",
       " 'partner',\n",
       " 'business',\n",
       " 'briefing',\n",
       " 'tomorrow',\n",
       " '-',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of',\n",
       " 'you',\n",
       " 'there',\n",
       " ':)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path=\"./\", format=\"csv\", \n",
    "    train='dataset.csv', test='dataset.csv',\n",
    "    skip_header = True,\n",
    "    fields=[('text', TEXT), ('labels', LABELS)])\n",
    "\n",
    "train_dataset[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train_dataset, test_dataset), batch_size=160,\n",
    "    sort_key = lambda x: len(x.text), device=device,\n",
    "    sort_within_batch = True, shuffle = True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion du vocabulaire et des word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, min_freq=2, vectors = 'glove.6B.50d')\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  556,    0,  ...,    0,    2,    5],\n",
       "        [   0,    0,  959,  ...,  128,  222,    0],\n",
       "        [  55,    8,  285,  ...,    0,    2,    8],\n",
       "        ...,\n",
       "        [  12,  163,    9,  ...,    0,   53,    1],\n",
       "        [2301,   86,   10,  ...,    2,  378,    1],\n",
       "        [1324,    0,    3,  ...,    0,    3,    1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModele(nn.Module):\n",
    "    def __init__(self, embedding_dim=50):\n",
    "        super(LSTMModele, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze = False)# une couche qui ne marche qu'avec les imports qui ne marchent pas...\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embedding_dim, 4) # 2 car pos neg\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs) # pour faire le lien entre indice et vecteur du mot associé\n",
    "        outputs, (h_n,c_n) = self.lstm(embeds)\n",
    "        x = h_n[0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMModele(embedding_dim = 50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModele(\n",
       "  (embeddings): Embedding(2697, 50)\n",
       "  (lstm): LSTM(50, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis code de BOUCLE d'APPRENTISSAGE + MESURE DES PERFORMANCES + accuracy_score IDENTIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "epoch : 1\n",
      "epoch : 2\n",
      "epoch : 3\n",
      "epoch : 4\n",
      "epoch : 5\n",
      "epoch : 6\n",
      "epoch : 7\n",
      "epoch : 8\n",
      "epoch : 9\n",
      "epoch : 10\n",
      "epoch : 11\n",
      "epoch : 12\n",
      "epoch : 13\n",
      "epoch : 14\n",
      "epoch : 15\n",
      "epoch : 16\n",
      "epoch : 17\n",
      "epoch : 18\n",
      "epoch : 19\n",
      "epoch : 20\n",
      "epoch : 21\n",
      "epoch : 22\n",
      "epoch : 23\n",
      "epoch : 24\n",
      "epoch : 25\n",
      "epoch : 26\n",
      "epoch : 27\n",
      "epoch : 28\n",
      "epoch : 29\n",
      "epoch : 30\n",
      "epoch : 31\n",
      "epoch : 32\n",
      "epoch : 33\n",
      "epoch : 34\n",
      "epoch : 35\n",
      "epoch : 36\n",
      "epoch : 37\n",
      "epoch : 38\n",
      "epoch : 39\n",
      "epoch : 40\n",
      "epoch : 41\n",
      "epoch : 42\n",
      "epoch : 43\n",
      "epoch : 44\n",
      "epoch : 45\n",
      "epoch : 46\n",
      "epoch : 47\n",
      "epoch : 48\n",
      "epoch : 49\n",
      "epoch : 50\n",
      "epoch : 51\n",
      "epoch : 52\n",
      "epoch : 53\n",
      "epoch : 54\n",
      "epoch : 55\n",
      "epoch : 56\n",
      "epoch : 57\n",
      "epoch : 58\n",
      "epoch : 59\n",
      "epoch : 60\n",
      "epoch : 61\n",
      "epoch : 62\n",
      "epoch : 63\n",
      "epoch : 64\n",
      "epoch : 65\n",
      "epoch : 66\n",
      "epoch : 67\n",
      "epoch : 68\n",
      "epoch : 69\n",
      "epoch : 70\n",
      "epoch : 71\n",
      "epoch : 72\n",
      "epoch : 73\n",
      "epoch : 74\n",
      "epoch : 75\n",
      "epoch : 76\n",
      "epoch : 77\n",
      "epoch : 78\n",
      "epoch : 79\n",
      "epoch : 80\n",
      "epoch : 81\n",
      "epoch : 82\n",
      "epoch : 83\n",
      "epoch : 84\n",
      "epoch : 85\n",
      "epoch : 86\n",
      "epoch : 87\n",
      "epoch : 88\n",
      "epoch : 89\n",
      "epoch : 90\n",
      "epoch : 91\n",
      "epoch : 92\n",
      "epoch : 93\n",
      "epoch : 94\n",
      "epoch : 95\n",
      "epoch : 96\n",
      "epoch : 97\n",
      "epoch : 98\n",
      "epoch : 99\n",
      "epoch : 100\n",
      "epoch : 101\n",
      "epoch : 102\n",
      "epoch : 103\n",
      "epoch : 104\n",
      "epoch : 105\n",
      "epoch : 106\n",
      "epoch : 107\n",
      "epoch : 108\n",
      "epoch : 109\n",
      "epoch : 110\n",
      "epoch : 111\n",
      "epoch : 112\n",
      "epoch : 113\n",
      "epoch : 114\n",
      "epoch : 115\n",
      "epoch : 116\n",
      "epoch : 117\n",
      "epoch : 118\n",
      "epoch : 119\n",
      "epoch : 120\n",
      "epoch : 121\n",
      "epoch : 122\n",
      "epoch : 123\n",
      "epoch : 124\n",
      "epoch : 125\n",
      "epoch : 126\n",
      "epoch : 127\n",
      "epoch : 128\n",
      "epoch : 129\n",
      "epoch : 130\n",
      "epoch : 131\n",
      "epoch : 132\n",
      "epoch : 133\n",
      "epoch : 134\n",
      "epoch : 135\n",
      "epoch : 136\n",
      "epoch : 137\n",
      "epoch : 138\n",
      "epoch : 139\n",
      "epoch : 140\n",
      "epoch : 141\n",
      "epoch : 142\n",
      "epoch : 143\n",
      "epoch : 144\n",
      "epoch : 145\n",
      "epoch : 146\n",
      "epoch : 147\n",
      "epoch : 148\n",
      "epoch : 149\n",
      "epoch : 150\n",
      "epoch : 151\n",
      "epoch : 152\n",
      "epoch : 153\n",
      "epoch : 154\n",
      "epoch : 155\n",
      "epoch : 156\n",
      "epoch : 157\n",
      "epoch : 158\n",
      "epoch : 159\n",
      "epoch : 160\n",
      "epoch : 161\n",
      "epoch : 162\n",
      "epoch : 163\n",
      "epoch : 164\n",
      "epoch : 165\n",
      "epoch : 166\n",
      "epoch : 167\n",
      "epoch : 168\n",
      "epoch : 169\n",
      "epoch : 170\n",
      "epoch : 171\n",
      "epoch : 172\n",
      "epoch : 173\n",
      "epoch : 174\n",
      "epoch : 175\n",
      "epoch : 176\n",
      "epoch : 177\n",
      "epoch : 178\n",
      "epoch : 179\n",
      "epoch : 180\n",
      "epoch : 181\n",
      "epoch : 182\n",
      "epoch : 183\n",
      "epoch : 184\n",
      "epoch : 185\n",
      "epoch : 186\n",
      "epoch : 187\n",
      "epoch : 188\n",
      "epoch : 189\n",
      "epoch : 190\n",
      "epoch : 191\n",
      "epoch : 192\n",
      "epoch : 193\n",
      "epoch : 194\n",
      "epoch : 195\n",
      "epoch : 196\n",
      "epoch : 197\n",
      "epoch : 198\n",
      "epoch : 199\n",
      "Finished Training\n",
      "Wall time: 31min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nb_epoch = 200\n",
    "for epoch in range(nb_epoch):\n",
    "    for i in range(0, train_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(train_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            \n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except:\n",
    "            print(\"done\")\n",
    "    print ('epoch : ' + str(epoch))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mesure des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# print(type(train_iter.batches))\n",
    "# print(type(test_iter.batches))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, train_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(train_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "        except:\n",
    "            print(\"error\")\n",
    "\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_preds = np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970482546201233"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(all_labels,all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
