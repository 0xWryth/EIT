{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch sur des séquences de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5883\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "filename = 'train_label_final.txt'\n",
    "lines = open(filename,\"r\", encoding='UTF-8').readlines()\n",
    "nb_lines = len(lines)\n",
    "nb_lines_half = math.ceil(nb_lines*0.7)\n",
    "# Avec le code actuel, comme on fait nb_lines/2 = nb_lines*0.5, la répartition entre les 2 fichiers est de 50%/50%\n",
    "# Ainsi, si on veut une répartition de 70%/30% pour jeu d'entrainement/validation, il suffit de faire nb_lines*0.7\n",
    "print(nb_lines_half)\n",
    "\n",
    "fout = open(\"output0.txt\",\"wt\", encoding='UTF-8')\n",
    "for i,line in enumerate(lines):\n",
    "    \n",
    "    if((i+1)%nb_lines_half) == 0:\n",
    "        line = str(line).replace('\\n', '')\n",
    "        \n",
    "    if (i%nb_lines_half) == 0:\n",
    "        if fout: fout.close()\n",
    "        fout = open('output%d.txt' % (i/nb_lines_half), 'w', encoding='UTF-8')\n",
    "        \n",
    "    fout.write(line)\n",
    "    \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./output0.txt\"\n",
    "path2 = \"./output1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_txt_to_csv(path, out):\n",
    "    f = open(path, \"r\", encoding=\"UTF-8\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    decisions = [\"pos\", \"neg\", \"neu\", \"irr\"]\n",
    "\n",
    "    # Future CSV content !\n",
    "    content = \"text,opinion\"\n",
    "\n",
    "    for l in lines:\n",
    "        # Removing empty lines\n",
    "        sl = l.split()\n",
    "        if len(sl) == 0:\n",
    "            continue\n",
    "\n",
    "        # Removing no consensus line\n",
    "        m = re.match(r\"[(](.*),(.*),(consensus)[)]\", sl[0])\n",
    "        if m is None:\n",
    "            continue\n",
    "\n",
    "        # Parsing opinion\n",
    "        try:\n",
    "            opinion = m.string.split(\",\")[1]\n",
    "            if opinion in decisions:\n",
    "                # Opinion string is converted to an integer value\n",
    "                index = decisions.index(opinion)\n",
    "\n",
    "                # Removing opinion\n",
    "                text = re.sub(r\"[(](.*),(.*),(consensus)[)]\", \"\", l)\n",
    "\n",
    "                # Removing comma since comma is CSV separator\n",
    "                text = text.replace(\",\", \" \")\n",
    "\n",
    "                # Remvoing extra \\n\n",
    "                text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "                # Removing extra space\n",
    "                text = ' '.join(text.split())\n",
    "\n",
    "                content = content + \"\\n\" + text + \",\" + str(index) \n",
    "            else:\n",
    "                # Found opinion is... Wrong !\n",
    "                continue\n",
    "        except:\n",
    "            # Opinion is not found on the line\n",
    "            continue\n",
    "\n",
    "    # Writing CSV content to out file\n",
    "    csv = open(out, \"w\", encoding=\"UTF-8\")\n",
    "    csv.write(content)\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_txt_to_csv(path, \"dataset.csv\")\n",
    "convert_txt_to_csv(path2, \"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext # pip3 install torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commandes utiles en cas de lib manquantes :\n",
    "\n",
    "# !pip3 install -U spacy                       # spaCy lib\n",
    "# !pip3 install torch torchvision torchaudio   # PyTorch (see <pytorch.org/get-started/locally> for GPU support)\n",
    "# !python3 -m spacy download en_core_web_trf    # download spaCy trained pipeline\n",
    "\n",
    "# et pour certains :\n",
    "# pip install transformers -U\n",
    "\n",
    "# et pour tester :\n",
    "# !python3 -m spacy download xx_ent_wiki_sm    # download spaCy trained pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir d'ici, changer les noms des titres... plus ou moins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# TODO: find a way to merge EN & FR language pipelines\n",
    "spacy_en = spacy.load('en_core_web_trf')\n",
    "#spacy_en = spacy.load('xx_ent_wiki_sm') # multi language to test --> plus nul. Voir comment utiliser 2 en meme temps plutot\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des prétraitements sur le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, lower = True, include_lengths = False,\n",
    "            pad_token = \"<pad>\", unk_token = \"<unk>\",\n",
    "            batch_first = True, tokenize = tokenizer)\n",
    "\n",
    "LABELS = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'london',\n",
       " '#',\n",
       " 'microsoft',\n",
       " 'partner',\n",
       " 'business',\n",
       " 'briefing',\n",
       " 'tomorrow',\n",
       " '-',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of',\n",
       " 'you',\n",
       " 'there',\n",
       " ':)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path=\"./\", format=\"csv\", \n",
    "    train='dataset.csv', test='dataset2.csv',\n",
    "    skip_header = True,\n",
    "    fields=[('text', TEXT), ('labels', LABELS)])\n",
    "\n",
    "train_dataset[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train_dataset, test_dataset), batch_size=160,\n",
    "    sort_key = lambda x: len(x.text), device=device,\n",
    "    sort_within_batch = True, shuffle = True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion du vocabulaire et des word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, min_freq=2, vectors = 'glove.6B.50d')\n",
    "#TEXT.build_vocab(train_dataset, min_freq=2, vectors = 'glove.6B.100d')\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  12,   59,  541,  ...,    0,   64, 1175],\n",
       "        [  15,  179,    7,  ...,    6,    2,  100],\n",
       "        [   5,    0,   36,  ...,  127,    2,    5],\n",
       "        ...,\n",
       "        [   2,    0,   41,  ...,    6,    1,    1],\n",
       "        [ 839,  979,  443,  ...,   32,    1,    1],\n",
       "        [   2,    5,   11,  ...,    0,    1,    1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModele(nn.Module):\n",
    "    def __init__(self, embedding_dim=50):\n",
    "    #def __init__(self, embedding_dim=100):\n",
    "        super(LSTMModele, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze = False)# une couche qui ne marche qu'avec les imports qui ne marchent pas...\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embedding_dim, 4) # 4 car {pos,neg,neu,irr} \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs) # pour faire le lien entre indice et vecteur du mot associé\n",
    "        outputs, (h_n,c_n) = self.lstm(embeds)\n",
    "        x = h_n[0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMModele(embedding_dim = 50).to(device)\n",
    "#net = LSTMModele(embedding_dim = 100).to(device)\n",
    "# embedding_dim = 50 car la taille de chaque vecteur de représentation des mots par l'embedding utilisé à partir de\n",
    "# la base du fichier \"glove.6B.50d\", est de taille 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModele(\n",
       "  (embeddings): Embedding(1981, 100)\n",
       "  (lstm): LSTM(100, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boucle d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "epoch : 1\n",
      "epoch : 2\n",
      "epoch : 3\n",
      "epoch : 4\n",
      "epoch : 5\n",
      "epoch : 6\n",
      "epoch : 7\n",
      "epoch : 8\n",
      "epoch : 9\n",
      "epoch : 10\n",
      "epoch : 11\n",
      "epoch : 12\n",
      "epoch : 13\n",
      "epoch : 14\n",
      "epoch : 15\n",
      "epoch : 16\n",
      "epoch : 17\n",
      "epoch : 18\n",
      "epoch : 19\n",
      "epoch : 20\n",
      "epoch : 21\n",
      "epoch : 22\n",
      "epoch : 23\n",
      "epoch : 24\n",
      "epoch : 25\n",
      "epoch : 26\n",
      "epoch : 27\n",
      "epoch : 28\n",
      "epoch : 29\n",
      "epoch : 30\n",
      "epoch : 31\n",
      "epoch : 32\n",
      "epoch : 33\n",
      "epoch : 34\n",
      "epoch : 35\n",
      "epoch : 36\n",
      "epoch : 37\n",
      "epoch : 38\n",
      "epoch : 39\n",
      "epoch : 40\n",
      "epoch : 41\n",
      "epoch : 42\n",
      "epoch : 43\n",
      "epoch : 44\n",
      "epoch : 45\n",
      "epoch : 46\n",
      "epoch : 47\n",
      "epoch : 48\n",
      "epoch : 49\n",
      "epoch : 50\n",
      "epoch : 51\n",
      "epoch : 52\n",
      "epoch : 53\n",
      "epoch : 54\n",
      "epoch : 55\n",
      "epoch : 56\n",
      "epoch : 57\n",
      "epoch : 58\n",
      "epoch : 59\n",
      "epoch : 60\n",
      "epoch : 61\n",
      "epoch : 62\n",
      "epoch : 63\n",
      "epoch : 64\n",
      "epoch : 65\n",
      "epoch : 66\n",
      "epoch : 67\n",
      "epoch : 68\n",
      "epoch : 69\n",
      "epoch : 70\n",
      "epoch : 71\n",
      "epoch : 72\n",
      "epoch : 73\n",
      "epoch : 74\n",
      "epoch : 75\n",
      "epoch : 76\n",
      "epoch : 77\n",
      "epoch : 78\n",
      "epoch : 79\n",
      "epoch : 80\n",
      "epoch : 81\n",
      "epoch : 82\n",
      "epoch : 83\n",
      "epoch : 84\n",
      "epoch : 85\n",
      "epoch : 86\n",
      "epoch : 87\n",
      "epoch : 88\n",
      "epoch : 89\n",
      "epoch : 90\n",
      "epoch : 91\n",
      "epoch : 92\n",
      "epoch : 93\n",
      "epoch : 94\n",
      "epoch : 95\n",
      "epoch : 96\n",
      "epoch : 97\n",
      "epoch : 98\n",
      "epoch : 99\n",
      "epoch : 100\n",
      "epoch : 101\n",
      "epoch : 102\n",
      "epoch : 103\n",
      "epoch : 104\n",
      "epoch : 105\n",
      "epoch : 106\n",
      "epoch : 107\n",
      "epoch : 108\n",
      "epoch : 109\n",
      "epoch : 110\n",
      "epoch : 111\n",
      "epoch : 112\n",
      "epoch : 113\n",
      "epoch : 114\n",
      "epoch : 115\n",
      "epoch : 116\n",
      "epoch : 117\n",
      "epoch : 118\n",
      "epoch : 119\n",
      "epoch : 120\n",
      "epoch : 121\n",
      "epoch : 122\n",
      "epoch : 123\n",
      "epoch : 124\n",
      "epoch : 125\n",
      "epoch : 126\n",
      "epoch : 127\n",
      "epoch : 128\n",
      "epoch : 129\n",
      "epoch : 130\n",
      "epoch : 131\n",
      "epoch : 132\n",
      "epoch : 133\n",
      "epoch : 134\n",
      "epoch : 135\n",
      "epoch : 136\n",
      "epoch : 137\n",
      "epoch : 138\n",
      "epoch : 139\n",
      "epoch : 140\n",
      "epoch : 141\n",
      "epoch : 142\n",
      "epoch : 143\n",
      "epoch : 144\n",
      "epoch : 145\n",
      "epoch : 146\n",
      "epoch : 147\n",
      "epoch : 148\n",
      "epoch : 149\n",
      "epoch : 150\n",
      "epoch : 151\n",
      "epoch : 152\n",
      "epoch : 153\n",
      "epoch : 154\n",
      "epoch : 155\n",
      "epoch : 156\n",
      "epoch : 157\n",
      "epoch : 158\n",
      "epoch : 159\n",
      "epoch : 160\n",
      "epoch : 161\n",
      "epoch : 162\n",
      "epoch : 163\n",
      "epoch : 164\n",
      "epoch : 165\n",
      "epoch : 166\n",
      "epoch : 167\n",
      "epoch : 168\n",
      "epoch : 169\n",
      "epoch : 170\n",
      "epoch : 171\n",
      "epoch : 172\n",
      "epoch : 173\n",
      "epoch : 174\n",
      "epoch : 175\n",
      "epoch : 176\n",
      "epoch : 177\n",
      "epoch : 178\n",
      "epoch : 179\n",
      "epoch : 180\n",
      "epoch : 181\n",
      "epoch : 182\n",
      "epoch : 183\n",
      "epoch : 184\n",
      "epoch : 185\n",
      "epoch : 186\n",
      "epoch : 187\n",
      "epoch : 188\n",
      "epoch : 189\n",
      "epoch : 190\n",
      "epoch : 191\n",
      "epoch : 192\n",
      "epoch : 193\n",
      "epoch : 194\n",
      "epoch : 195\n",
      "epoch : 196\n",
      "epoch : 197\n",
      "epoch : 198\n",
      "epoch : 199\n",
      "End of training\n",
      "Wall time: 41min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nb_epoch = 200\n",
    "for epoch in range(nb_epoch):\n",
    "    for i in range(0, train_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(train_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            \n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except:\n",
    "            print(\"done\")\n",
    "    print ('epoch : ' + str(epoch))\n",
    "\n",
    "print('End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mesure des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# print(type(train_iter.batches))\n",
    "# print(type(test_iter.batches))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(test_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "        except:\n",
    "            print(\"error\")\n",
    "\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_preds = np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(all_labels,all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.675 --> glove50d\n",
    "0.64 --> glove100d mdr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
