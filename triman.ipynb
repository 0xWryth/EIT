{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76adb919",
   "metadata": {},
   "source": [
    "## Pytorch sur des séquences de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdefacf",
   "metadata": {},
   "source": [
    "#### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be527ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext # pip install torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb4ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_trf')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01d9c",
   "metadata": {},
   "source": [
    "#### Définition des prétraitements sur le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfe159b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, lower = True, include_lengths = False,\n",
    "            pad_token = \"<pad>\", unk_token = \"<unk>\",\n",
    "            batch_first = True, tokenize = tokenizer)\n",
    "\n",
    "LABELS = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933442dc",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cf5e7",
   "metadata": {},
   "source": [
    "#### Création des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e2671b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'london',\n",
       " '#',\n",
       " 'microsoft',\n",
       " 'partner',\n",
       " 'business',\n",
       " 'briefing',\n",
       " 'tomorrow',\n",
       " '-',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of',\n",
       " 'you',\n",
       " 'there',\n",
       " ':)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path=\"./\", format=\"csv\", \n",
    "    train='toto2.csv', test='toto2.csv',\n",
    "    skip_header = True,\n",
    "    fields=[('text', TEXT), ('labels', LABELS)])\n",
    "\n",
    "train_dataset[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5a760",
   "metadata": {},
   "source": [
    "#### Gestion des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train_dataset, test_dataset), batch_size=160,\n",
    "    sort_key = lambda x: len(x.text), device=device,\n",
    "    sort_within_batch = True, shuffle = True, repeat=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98c16c",
   "metadata": {},
   "source": [
    "#### Gestion du vocabulaire et des word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcdcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, min_freq=2, vectors = 'glove.6B.50d')\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5cd5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  12,   65, 7262,  ...,  116,  175,   10],\n",
       "        [  17, 3079,    2,  ...,    2,    8,    3],\n",
       "        [7692,  290, 3699,  ...,  325,    2,  299],\n",
       "        ...,\n",
       "        [2947,   12,  108,  ...,  852, 5564,    1],\n",
       "        [2903, 1441,   10,  ..., 4551,  510,    1],\n",
       "        [4448,  174,  401,  ...,   38,    4,    1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6045afa",
   "metadata": {},
   "source": [
    "#### Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1554fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModele(nn.Module):\n",
    "    def __init__(self, embedding_dim=50):\n",
    "        super(LSTMModele, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze = False)# une couche qui ne marche qu'avec les imports qui ne marchent pas...\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = embedding_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embedding_dim, 4) # 2 car pos neg\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs) # pour faire le lien entre indice et vecteur du mot associé\n",
    "        outputs, (h_n,c_n) = self.lstm(embeds)\n",
    "        x = h_n[0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641a4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMModele(embedding_dim = 50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2f1982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModele(\n",
       "  (embeddings): Embedding(8916, 50)\n",
       "  (lstm): LSTM(50, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7284ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dd9af",
   "metadata": {},
   "source": [
    "Puis code de BOUCLE d'APPRENTISSAGE + MESURE DES PERFORMANCES + accuracy_score IDENTIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74cb3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "epoch : 0\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "epoch : 1\n",
      "done\n",
      "done\n",
      "done\n",
      "epoch : 2\n",
      "done\n",
      "epoch : 3\n",
      "epoch : 4\n",
      "Finished Training\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nb_epoch = 5\n",
    "for epoch in range(nb_epoch):\n",
    "    for i in range(0, train_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(train_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            \n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except:\n",
    "            print(\"done\")\n",
    "    print ('epoch : ' + str(epoch))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726808a",
   "metadata": {},
   "source": [
    "#### Mesure des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5fd55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# print(type(train_iter.batches))\n",
    "# print(type(test_iter.batches))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, train_iter.batch_size):\n",
    "        try:\n",
    "            batch = next(iter(train_iter))\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "        except:\n",
    "            print(\"error\")\n",
    "\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_preds = np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c2ec2",
   "metadata": {},
   "source": [
    "#### Without Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "858b9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8665096034820109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(all_labels,all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
