{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOT3u0GZKWc8"
   },
   "source": [
    "# Réseau de neurones: les bases en numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Irgrskx6KWc-"
   },
   "source": [
    "Le but de ce TP est de voir les bases des réseaux de neurones en numpy. Puis de voir comment en pratique on s'en sert avec la librairie Pytorch.\n",
    "\n",
    "Ce TP sera **à rendre** (voir Discord).\n",
    "Il y a n points dans ce TP, qui seront divisé pour obtenir une note sur 4.\n",
    "\n",
    "Ce TP est à faire avec votre **groupe**. Rendez-le avec votre **groupe**.\n",
    "\n",
    "Si vous avez des **questions**, n'hésitez pas à me les poser sur **Discord**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlmi-culKWdA"
   },
   "source": [
    "## Lecture des données, tokenization et BoW\n",
    "\n",
    "Dans cette section vous devez lire les données (seulement les consensus).\n",
    "\n",
    "Vous devez construire votre tokenizer (et de préférence le sauvegarder).\n",
    "\n",
    "Vous devez transformer votre jeu de données (liste de tweets et labels) en liste de vecteurs BoW + liste d'indice des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Ekg9fkKWdD"
   },
   "outputs": [],
   "source": [
    "# Les imports sont préparé ici\n",
    "# n'enlevez pas les % car il permettent le reload de modules ou l'affichage dans le notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle as pkl\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 1** 1pt.\n",
    "\n",
    "Importez/réécrivez le code permettant de lire vos données et les transformer en BoW.\n",
    "\n",
    "A la fin de cette cellule, vous devrez avoir trois variables d'instanciées :\n",
    "- X : la liste des vecteurs BoW de vos tweets\n",
    "- Y_one_hot : la liste des indices des labels de vos tweets (Y_one_hot[i] doit contenir le label de X[i])\n",
    "- Y_one_hot : la liste des labels sous forme one_hot (c'est similaire à un BoW, vous prenez un vecteur de zeros et mettez un 1 à l'indice du label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G74xriNqIWKU"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path, consensus=False):\n",
    "    f = open(path, 'r', encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    corpus = []\n",
    "    for l in lines:\n",
    "        sl = l.split()\n",
    "        if len(sl) == 0:\n",
    "            continue\n",
    "        if consensus:\n",
    "            m = re.match(r\"[(](.*),(.*),(consensus)[)]\", sl[0])\n",
    "            if m is not None:\n",
    "                corpus.append([sl[1:], sl[0]])\n",
    "        else:\n",
    "            corpus.append([sl[1:], sl[0]])\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "class WordTokenizer:\n",
    "    def __init__(self, bos='BOS', eos='EOS', unk='UNK', pad='PAD'):\n",
    "\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "\n",
    "        self.word2id = {pad: 0,\n",
    "                        unk: 1,\n",
    "                        bos: 2,\n",
    "                        eos: 3}\n",
    "\n",
    "        self.id2word = {0: pad,\n",
    "                        1: unk,\n",
    "                        2: bos,\n",
    "                        3: eos}\n",
    "\n",
    "    def add_to_voc(self, sent):\n",
    "        \"\"\"\n",
    "        :param sentence: string\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for w in sent:\n",
    "            if w not in self.word2id.keys():\n",
    "                self.word2id[w] = len(self.id2word.keys())\n",
    "                self.id2word[self.word2id[w]] = w\n",
    "\n",
    "    def str_to_ids(self, sentence):\n",
    "        sent = sentence.split()\n",
    "        ret = []\n",
    "        for w in sent:\n",
    "            if w not in self.word2id.keys():\n",
    "                ret.append(self.word2id[self.unk])\n",
    "            else:\n",
    "                ret.append(self.word2id[w])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def words_to_ids(self, sent):\n",
    "        ret = []\n",
    "        for w in sent:\n",
    "            if w not in self.word2id.keys():\n",
    "                ret.append(self.word2id[self.unk])\n",
    "            else:\n",
    "                ret.append(self.word2id[w])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            words.append(self.id2word[i])\n",
    "        return words\n",
    "\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, bos='<B>', eos='<E>', unk='<U>', pad='<P>'):\n",
    "\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "\n",
    "        self.char2id = {pad: 0,\n",
    "                        unk: 1,\n",
    "                        bos: 2,\n",
    "                        eos: 3}\n",
    "\n",
    "        self.id2char = {0: pad,\n",
    "                        1: unk,\n",
    "                        2: bos,\n",
    "                        3: eos}\n",
    "\n",
    "    def add_to_voc(self, sentence):\n",
    "        \"\"\"\n",
    "        adds vocabulary (chars) found in sentence to the dictionaries\n",
    "        :param sentence: string\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for w in sentence:\n",
    "            if w not in self.char2id.keys():\n",
    "                self.char2id[w] = len(self.id2char.keys())\n",
    "                self.id2char[self.char2id[w]] = w\n",
    "\n",
    "    def chars_to_ids(self, sentence):\n",
    "        ret = []\n",
    "        for c in sentence:\n",
    "            if c not in self.char2id.keys():\n",
    "                ret.append(self.char2id[self.unk])\n",
    "            else:\n",
    "                ret.append(self.char2id[c])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def ids_to_chars(self, ids):\n",
    "        chars = \"\"\n",
    "        for i in ids:\n",
    "            chars += self.id2char[i]\n",
    "        return chars\n",
    "\n",
    "\n",
    "class TrigramTokenizer:\n",
    "    def __init__(self, bos='<B>', eos='<E>', unk='<U>', pad='<P>'):\n",
    "\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "\n",
    "        self.char2id = {pad: 0,\n",
    "                        unk: 1,\n",
    "                        bos: 2,\n",
    "                        eos: 3}\n",
    "\n",
    "        self.id2char = {0: pad,\n",
    "                        1: unk,\n",
    "                        2: bos,\n",
    "                        3: eos}\n",
    "\n",
    "    def add_to_voc(self, sentence):\n",
    "        \"\"\"\n",
    "        adds vocabulary (chars) found in sentence to the dictionaries\n",
    "        :param sentence: string\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for i in range(len(sentence)-3):\n",
    "            if sentence[i:i+3] not in self.char2id.keys():\n",
    "                self.char2id[sentence[i:i+3]] = len(self.id2char.keys())\n",
    "                self.id2char[self.char2id[sentence[i:i+3]]] = sentence[i:i+3]\n",
    "\n",
    "    def chars_to_ids(self, sentence):\n",
    "        ret = []\n",
    "        for i in range(len(sentence)-3):\n",
    "            if sentence[i:i+3] not in self.char2id.keys():\n",
    "                ret.append(self.char2id[self.unk])\n",
    "            else:\n",
    "                ret.append(self.char2id[sentence[i:i+3]])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def ids_to_chars(self, ids):\n",
    "        chars = \"\"\n",
    "        for i in ids:\n",
    "            chars += self.id2char[i]\n",
    "        return chars\n",
    "\n",
    "def list_to_bow(sent, dic):\n",
    "    \"sent list of ids, dic: dictionary {word, index}\"\n",
    "    vec = np.zeros(len(dic), dtype=int)\n",
    "    for i in sent:\n",
    "        vec[i] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def list_to_bow_freq(sent, dic):\n",
    "    vec = np.zeros(len(dic))\n",
    "    for i in sent:\n",
    "        vec[i] += 1\n",
    "    for i in range(len(vec)):\n",
    "        vec[i] = vec[i] / len(sent)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def n_grams(sents, n):\n",
    "\n",
    "    new_sents = []\n",
    "    dic = {}\n",
    "\n",
    "    for s in sents:\n",
    "        ns = []\n",
    "        for i in range(len(s)-(n-1)):\n",
    "            t = \"\"\n",
    "            for j in range(n):\n",
    "                t += f\"{s[i+j]} \"\n",
    "            t = t[:-1]\n",
    "            ns.append(t)\n",
    "            if t not in dic:\n",
    "                dic[t] = len(dic.keys())\n",
    "\n",
    "        new_sents.append(ns)\n",
    "    return new_sents, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettez votre code ici\n",
    "# path = \"C:/Users/Lucas/Downloads/train_label_final.txt\"\n",
    "path = \"train_label_final.txt\"\n",
    "\n",
    "corpus = read_corpus(path, consensus=True)\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "labels = {'pos': 0, 'neg': 1, 'neu': 2, 'irr': 3}\n",
    "Y_one_hot = [] # Pour les labels il faut des vecteurs en one_hot, par exemple pour irr : [0,0,0,1]\n",
    "Y = []\n",
    "for i in range(len(corpus)):\n",
    "    lab = re.match(r\"[(].*,(.*),.*[)]\", corpus[i][1])\n",
    "    ap = np.zeros(len(labels.keys()))\n",
    "    ap[labels[lab[1].lower()]] = 1\n",
    "    Y.append(labels[lab[1].lower()])\n",
    "    Y_one_hot.append(ap)\n",
    "\n",
    "\n",
    "tweets = [c[0] for c in corpus]\n",
    "for t in tweets:\n",
    "    tokenizer.add_to_voc(t)\n",
    "\n",
    "    \n",
    "tweets_ids = [tokenizer.words_to_ids(t) for t in tweets]\n",
    "\n",
    "\n",
    "X = [list_to_bow(tw, tokenizer.word2id) for tw in tweets_ids]\n",
    "X_freq = [list_to_bow_freq(tw, tokenizer.word2id) for tw in tweets_ids]\n",
    "\n",
    "\n",
    "print(X[0]) # x sont les tweets transformés en vecteurs BoW\n",
    "print(len(X[0]))\n",
    "print(X_freq[0])\n",
    "print(len(X_freq[0]))\n",
    "print(Y_one_hot[0]) # y sont les labels transformés en indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvZIFXNNKWdV"
   },
   "source": [
    "## Création du modèle\n",
    "\n",
    "Nous allons d'abord créer une couche linéaire.\n",
    "Celle ci comprendra le terme de biais.\n",
    "\n",
    "Rappel de la formule de la couche linéaire: \n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{W}\\mathbf{x}+ \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Notons *n_in* et *n_out* respectivement les dimensions de $\\mathbf{x}$ et $\\mathbf{y}$. \n",
    "\n",
    "**Tache 2** 1pt.\n",
    "- Coder la fonction d'initialisation suivante, l'initialisation est aléatoire Gaussien centrée en 0 avec un écart type 1 / sqrt(n_in). La fonction retourne W et b. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzcMLu1KKWdx"
   },
   "source": [
    "## Algorithme d'apprentissage \n",
    "\n",
    "On a tout ce qu'il faut pour mettre en oeuvre l'apprentissage d'un modèle simple. Le modèle est simplement une couche neuronale de sortie, sans couche cachée. \n",
    "\n",
    "L'algorithme se déroule en 2 temps, tout d'abord la préparation: \n",
    "- init. du modèle\n",
    "- préparation des données et des variables permettant de stocker l'historique d'apprentissage\n",
    "- init. des paramètres de la SGD\n",
    "- définir le nombre d'époque comme une variable\n",
    "\n",
    "Puis vient la boucle d'apprentissage qui pour chaque époque effectue pour chaque exemple d'apprentissage : \n",
    "- inférence du modèle sur l'exemple d'apprentissage \n",
    "- calcul de la contribution de l'exemple à la  fonction objectif, et également au taux d'erreur de classification\n",
    "- Calcul du gradient de sortie\n",
    "- Mise à jour du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 :  PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette section est de répliquer ce que vous avez fait à la main grâce à une librairie spécialisée.\n",
    "\n",
    "Cette librairie est Pytorch.\n",
    "\n",
    "Dans cette librairie vous retrouverez la gestion des poids sous forme de couches dans le sous-module nn.\n",
    "La gestion du gradient est quasiment automatique.\n",
    "\n",
    "Le jeu de données sera toujours les vecteurs BoW obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:1500]\n",
    "train_Y = Y_one_hot[:1500]\n",
    "train_Y_nhot = Y[:1500]\n",
    "\n",
    "valid_X = X[1500:]\n",
    "valid_Y = Y_one_hot[1500:]\n",
    "valid_Y_nhot = Y[1500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#device = 'cuda' if T.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 11** 3 pt\n",
    "\n",
    "Implémentez le même modèle que précédemment.\n",
    "\n",
    "Utilisez les couches Linear et LogSoftmax (les probabilités sont souvent des logs probabilités).\n",
    "Cherchez dans la documentation de torch.nn pour les couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): # pour faire un modèle dans pytorch il faut instancier la classe nn.Module\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.embeding = nn.Embedding(in_dim, out_dim)\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim) # Ici se trouve la couche Wx + b\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Pour pytorch, la plupars des fonction fonctionne avec des logarithmes\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Quelque soit le modèle, il vous faut une fonction forward\n",
    "        # Pour calculer la sortie d'une couche : y = couche(x) avec x un tensor\n",
    "        #embeds = self.embeding(inputs.to(T.int64))\n",
    "        # print(inputs.shape)\n",
    "        y = T.tanh(self.linear1(inputs))\n",
    "        preds = self.softmax(y)\n",
    "        return preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 12** 2pt\n",
    "\n",
    "Complétez la boucle d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    return T.stack(transposed_data[0], 0), T.stack(transposed_data[1], 0)\n",
    "\n",
    "def train(train_X, train_Y, valid_X, valid_Y, epochs=100, batch_size=64, lr = 1e-3):\n",
    "    model = Model(len(train_X[0]), 4)\n",
    "    \n",
    "    opti = T.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss() # Pour calculer la crossentropy, il faut calculer la NLLL après un logsofmax\n",
    "    \n",
    "    \n",
    "    ## Transformation des données pour l'entraînement\n",
    "    #trn_X = [T.tensor(x, dtype=T.long) for x in train_X]\n",
    "    #trn_Y = [T.tensor(y, dtype=T.long) for y in train_Y]\n",
    "    \n",
    "    #vld_X = [T.tensor(x, dtype=T.long) for x in valid_X]\n",
    "    #vld_Y = [T.tensor(y, dtype=T.long) for y in valid_Y]\n",
    "    \n",
    "    trn_X = T.tensor(train_X, dtype=T.float)\n",
    "    trn_Y = T.tensor(train_Y, dtype=T.long)\n",
    "    \n",
    "    vld_X = T.tensor(valid_X, dtype=T.float)\n",
    "    vld_Y = T.tensor(valid_Y, dtype=T.long)\n",
    "    \n",
    "    train_set = data.TensorDataset(trn_X, trn_Y)\n",
    "    valid_set = data.TensorDataset(vld_X, vld_Y)\n",
    "    \n",
    "    \n",
    "    ## Creation des loaders\n",
    "    train_sampler = data.BatchSampler(data.RandomSampler(range(len(train_X))), batch_size, False)\n",
    "    valid_sampler = data.BatchSampler(data.SequentialSampler(range(len(valid_X))), len(valid_X), False)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_set, batch_sampler=train_sampler, collate_fn=collate)\n",
    "    valid_loader = data.DataLoader(valid_set, batch_sampler=valid_sampler, collate_fn=collate)\n",
    "    \n",
    "    \n",
    "    losses = []\n",
    "    f1_valid = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        model.train() #passe votre modele en phase d'entrainement \n",
    "        \n",
    "        for batch_ndx, (trn_x, trn_y) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            \n",
    "            preds = model(trn_x)\n",
    "            loss = criterion(preds, trn_y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "        opti.zero_grad()\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_ndx, (vld_x, vld_y) in enumerate(valid_loader):\n",
    "            \n",
    "            preds_val = model(vld_x)\n",
    "            preds = T.argmax(preds_val, dim=1)\n",
    "            f1_valid.append(f1_score(vld_y.to('cpu').numpy(), preds.to('cpu').numpy(), average='micro',\n",
    "                            labels=[i for i in range(4)]))\n",
    "        print(f\"F1 {e}/{epochs}: {f1_valid[-1]}\")\n",
    "    \n",
    "    return model, losses, f1_valid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, losses, f1_valid = train(train_X, train_Y_nhot, valid_X, valid_Y_nhot, epochs=200, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x/(len(losses)/100) for x in range(len(losses))],losses, label=\"loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1_valid, label='f1')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "bestIndex = np.argmax(f1_valid)\n",
    "bestNC, bestScore = np.arange(0, 200, 1)[bestIndex], f1_valid[bestIndex]\n",
    "plt.plot(bestNC, f1_valid[bestIndex], marker='X', color='green')\n",
    "plt.title(\"Best score: ~{0:.1%}\".format(bestScore))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est super! Vous avez fini. N'hésitez pas à utiliser ce modèle comme base pour votre projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des performances par rapport à un SVM de type classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier d'initialiser train_X, train_Y_nhot, valid_X et valid_Y_nhot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend un kernel linéaire\n",
    "kernel = 'linear'\n",
    "\n",
    "linear_training_score = []\n",
    "linear_valid_score = []\n",
    "Cvalues = []\n",
    "\n",
    "clf = SVC(kernel=kernel)\n",
    "\n",
    "for C in [10**n for n in range(-5,6,1)]:\n",
    "    Cvalues.append(C)\n",
    "    clf.set_params(C=C)\n",
    "    clf.fit(train_X, train_Y_nhot)\n",
    "    \n",
    "    pred_train_Y = clf.predict(train_X)\n",
    "    pred_valid_Y = clf.predict(valid_X)\n",
    "    \n",
    "    trainscore = f1_score(train_Y_nhot, pred_train_Y, average='micro', labels=[i for i in range(4)])\n",
    "    validscore = f1_score(valid_Y_nhot, pred_valid_Y, average='micro', labels=[i for i in range(4)])\n",
    "    \n",
    "    linear_training_score.append(trainscore)\n",
    "    linear_valid_score.append(validscore)\n",
    "    \n",
    "    print(\"C=\", C, \"   training score:\", trainscore, \"   valid score:\", validscore)\n",
    "    \n",
    "\n",
    "plt.title(\"F1 score en fonction de la valeur du paramètre C\")\n",
    "plt.semilogx(Cvalues, linear_training_score, label= \"train score\")\n",
    "plt.plot(Cvalues, linear_valid_score, label= \"valid score\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe l'hyperparamètre C à 0.01 car au délà on a de l'overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.set_params(C=0.01)\n",
    "clf.fit(train_X, train_Y_nhot)\n",
    "pred_valid_Y = clf.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice = confusion_matrix(valid_Y_nhot, pred_valid_Y)\n",
    "df_cm = DataFrame(matrice, index = ['pos', 'neg', 'neu', 'irr'],\n",
    "                     columns = ['pos', 'neg', 'neu', 'irr'])\n",
    "plt.figure(figsize = (10,7))\n",
    "heatmap(df_cm, annot=True)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
